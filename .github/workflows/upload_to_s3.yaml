name: Deploy to s3

on: 
  push: 
    branches:
      - main
      - master
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      - name: checkout repo
        uses: actions/checkout@v3

      - name: set up python
        uses: actions/setup-python@v4
        with: 
          python-version: 3.10.9

      # - name: install dependencies
      #   run: pip install -r requirements.txt

      - name: zip spark jobs
        run: |
          zip -r spark_job.zip spark_job

      - name: configure aws credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-west-1

      - name: upload data to s3
        run: |
          aws s3 cp data/data.csv s3://emr-project-raw/serverless_example/data/

      - name: upload spark job to s3
        run: |
          aws s3 cp spark_job.zip s3://emr-project-raw/serverless_example/

      - name: upload spark job file to s3
        run: |
          aws s3 cp spark_job/spark_processor.py s3://emr-project-raw/serverless_example/
          
      # - name: upload dags  to s3
      #   run: |
      #     aws s3 cp dags/ s3://emr-project-raw/serverless_example/dags/ --recursive